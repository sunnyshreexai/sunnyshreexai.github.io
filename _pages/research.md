---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

As an AI Research Scientist, my work focuses on advancing the capabilities of **Large Language Models (LLMs)**, **Generative AI**, and **Natural Language Processing (NLP)** to build intelligent, scalable, and deployable systems. I bridge cutting-edge research with real-world applications—developing models and frameworks that make AI more adaptive, reliable, and trustworthy in production environments.

I specialize in **transformer-based architectures**, **foundation model development**, and **LLM fine-tuning**, with a focus on practical innovations that improve efficiency, robustness, and interpretability across diverse domains.

## Core Focus Areas

**Large Language Models & Generative AI** Designing and fine-tuning LLMs through supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and instruction tuning. My work emphasizes production-ready generative models optimized for accuracy, latency, and scalability.

**Generative Architectures** Working with transformers, diffusion models, GANs, and VAEs to create multimodal and text-to-X generation pipelines for content synthesis, simulation, and creative AI applications.

**Model Training & Optimization** Applying advanced training strategies—including transfer learning, knowledge distillation, and parameter-efficient fine-tuning—to maximize model performance while reducing compute cost and deployment overhead.

**AI Interpretability & Transparency** Developing explainability tools for model introspection through influence analysis, feature attribution, and attention visualization, enabling developers and stakeholders to better understand and trust model decisions.
